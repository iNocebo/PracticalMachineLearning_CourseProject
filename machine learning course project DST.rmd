---
title: 'Machine Learning: Peer Assessment'
output:
  html_document:
    keep_md: yes
  pdf_document: default
---

Author: iNocebo | Dominik St√§mpfli  
Course: predmachlearn-033, October 2015 
 
## Executive Summary 
In this project we created a machine learning algorithm capable of distinguishing the quality of exercise execution. The prediction is based on measurements of devices applied to the belt, forearm, arm, and dumbbell during exercise execution. The random forest model with 53 predictors is capable to predict the class of execution with an out of sample error of 0.79 %. 
 
## Background 
The algorithm is trained on a dataset provided by [Velloso et al.](http://groupware.les.inf.puc-rio.br/har) (see Credits) For this dataset "six young health participants were asked to perform one set of 10 repetitions of the Unilateral Dumbbell Biceps Curl in five different fashions: exactly according to the specification (Class A), throwing the elbows to the front (Class B), lifting the dumbbell only halfway (Class C), lowering the dumbbell only halfway (Class D) and throwing the hips to the front (Class E). Class A corresponds to the specified execution of the exercise, while the other 4 classes correspond to common mistakes. Participants were supervised by an experienced weight lifter to make sure the execution complied to the manner they were supposed to simulate. The exercises were performed by six male participants aged between 20-28 years, with little weight lifting experience." [[Groupware@LES](http://groupware.les.inf.puc-rio.br/har)] 
The goal of this project was to build a model capable of distinguishing the different quality classes of unilateral dumbbell biceps curls. This report describes the exploratory data analysis, dataset processing, cross validation, model building process, model comparison and final results on the testing set. 
 
## Methods 
For this project we used RStudio version 0.98.1103 with R version 3.2.2 on Windows 7 Professional. The libraries used throughout the project were {caret} and {dplyr} (see Credits). 
 
```{r load libraries, echo=TRUE, message=FALSE, warning=FALSE}
library(caret)
library(dplyr)
```

## Exploratory Data Analysis and Data Cleaning
A first quick look at the data with its 160 variables and 19'622 observations revealed features and single values which needed processing before we were able to use the dataset. A lot of processing could be done by adjusting the read-in: The error messages from Microsoft Excel *#DIV/0!*, the *NAs* represented as characters, and the empty obervations could be tackled with the argument *na.strings*. We processed the provided testing data as validation set at the same time as we wanted to use cross validation. 
 
```{r read in, echo=TRUE, message=FALSE, warning=FALSE}
training <- read.csv("pml-training.csv", na.strings = c("#DIV/0!", "NA", ""))
validation <- read.csv("pml-testing.csv", na.strings = c("#DIV/0!", "NA", ""))
```
 
The training set contains summarized and processed data from the original researchers within certain rows (visible as *"yes"* in the variable *new_window*) which is not present in the validation set. Hence we were obliged to exclude the corresponding rows. Excluding the summary rows left the dataset with certain columns only containing *NAs*, which may be excluded as well. Furthermore we regarded the time variables as measurements only applicable to the specific setting, not providing any prediction value. We excluded the corresponding columns from further processing. We wanted to make sure that each observation except for the user names and the classe factors is represented as numeric, not hindering the algorithm. This was achieved by applying a for-loop to the dataset. 
 
```{r set seed and preprocessing, echo=TRUE, message=FALSE, warning=FALSE}
set.seed(1337)
## removing the summary columns
training_subset <- filter(training, new_window == "no")
## removing the time variables
training_subset <- training_subset[-c(1, 3, 4, 5, 6, 7)]
## transforming each value into numeric except for user_name and classe
for(i in c(3:ncol(training_subset)-1)) {
        training_subset[,i] = as.numeric(as.character(training_subset[,i]))
}
## defining columns with NAs
nona_columns <- colnames(training_subset[colSums(is.na(training_subset)) == 0])
training_nona <- training_subset[nona_columns]

## repeating the same procedure for the validation set.
validation_subset <- filter(validation, new_window == "no")
validation_subset <- validation_subset[-c(1, 3, 4, 5, 6, 7)]
for(i in c(3:ncol(validation_subset)-1)) {
        validation_subset[,i] = as.numeric(as.character(validation_subset[,i]))
}
nona_columns_validation <- colnames(validation_subset[colSums(is.na(validation_subset)) == 0])
validation_nona <- validation_subset[nona_columns_validation]
```
 
## Cross Validation 
We created a partition within the training set for cross validation of our models as we wanted to avoid overfitting. **We used 75 % of the training data** to create the *cross_train* training subset and the other 25 % as the *cross_test* testing subset, leaving the provided testing set as validation set aside untouched. 
 
```{r partition creation - cross validation, echo=TRUE, message=FALSE, warning=FALSE}
crossVal <- createDataPartition(y = training_nona$classe, p = 0.75, list = F)
cross_train <- training_nona[crossVal, ]
cross_test <- training_nona[-crossVal, ]
dim(cross_train); dim(cross_test)
```
 
## The Final Model 
For our final model with its 53 predictors we used the packet {caret} (see Credits) and the **Random Forest** method, which bootstraps the data and splits the variables into groups by checking for their homogeneity. **No preprocessing was applied**. Due to the chosen method and the use of {caret}, the model calculation was very slow, but unbeaten by its accuracy of 0,989 within *cross_train* and 0.992 within *cross_test*, providing an **in sample error of 1.1 %** (1 - accuracy) and an **out of sample error of 0.79 %**. 
 
```{r model calculation presentation, echo=TRUE, message=FALSE, warning=FALSE, eval=FALSE}
model_rf <- train(classe ~ ., data = cross_train, method = "rf")
```
```{r model evaluation, echo=FALSE, message=FALSE, warning=FALSE}
model_rf <- readRDS("model_rf_DST.rds")
```
 
```{r model presentation, echo=FALSE, message=FALSE, warning=FALSE}
print(model_rf)
```
 
```{r confusionMatrix, echo=TRUE, message=FALSE, warning=FALSE}
pred_rf <- predict(model_rf, newdata = cross_test[-54])
confusionMatrix(pred_rf, cross_test[, 54])
```
 
The features are not equally important for the model. According to the plot shown below, user_name is the least important feature in our model. 

```{r varImp, echo=TRUE, message=FALSE, warning=FALSE}
importance <- varImp(model_rf, scale = FALSE)
plot(importance)
```
 
## Testing the Model against the Validation Set ("Testing" within the Course Project)
20 samples (read "exercise executions") had to be classified with the use of our model. The same read-in and processing were already applied to the validation set. The last column of the validation set had to be excluded as it represents the problem id only applicable to the test and not represented in the training dataset.  
```{r testing presentation, echo=TRUE, message=FALSE, warning=FALSE}
pred_val <- predict(model_rf, validation_nona[-54])
print(pred_val)
```
 
## Credits 
- Project: This project is part of Coursera's Practical Machine Learning within the "[Data Science](https://www.coursera.org/specializations/jhudatascience)" specialisation by Jeff Leek, PhD, Roger D. Peng, PhD, Brian Caffo, PhD, of Johns Hopkins University Bloomberg School of Public Health.
- [R](http://www.R-project.org): R Development Core Team (2008). R: A language and environment for statistical computing. R Foundation for Statistical Computing, Vienna, Austria. ISBN 3-900051-07-0 
- RStudio: Version 0.98.1103 
- [Weight Lifting Exercises Dataset](http://groupware.les.inf.puc-rio.br/har): Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013.
- {[ggplot2](http://had.co.nz/ggplot2/book)}: H. Wickham. ggplot2: elegant graphics for data analysis. Springer New York, 2009. 
- {[caret]((http://CRAN.R-project.org/package=caret))}: Max Kuhn. Contributions from Jed Wing, Steve Weston, Andre Williams,   Chris Keefer, Allan Engelhardt, Tony Cooper, Zachary Mayer, Brenton   Kenkel, the R Core Team, Michael Benesty, Reynald Lescarbeau, Andrew Ziem, Luca Scrucca, Yuan Tang and Can Candan. (2015). caret: Classification and Regression Training. R package version 6.0-57.
- {[dplyr](http://CRAN.R-project.org/package=dplyr)}: Hadley Wickham and Romain Francois (2015). dplyr: A Grammar of Data   Manipulation. R package version 0.4.3.
- {knitr}: Yihui Xie (2015). knitr: A General-Purpose Package for Dynamic Report Generation in R. R package
  version 1.9. 
 
## Other Models 
We tried to reduce the cross_train dataset to reduce the time of Random Forest model calculation. The models never achieved the same accuracy as our final model with 53 predictors: 
 
- The mean was calculated for each value according to groups of users and the quality of their execution (summarizing the dataset): 53 predictors with 30 samples; accuracy = 0.355. 
```{r summarizing, echo=TRUE, message=FALSE, warning=FALSE}
summarized <- aggregate(cross_train, by= list(cross_train$user_name,
                                              cross_train$classe), FUN = mean)
summarized$user_name <- summarized$Group.1
summarized$classe <- summarized$Group.2
summarized <- summarized[-c(1, 2)]
```
 
- Eliminating redundant (correlation > 0.5) features with {caret}'s *findCorrelation*: 22 predictors with 14414 samples; accuracy: 0.969. 
```{r redundant variables 50, echo=TRUE, message=FALSE, warning=FALSE}
correlationMatrix <- cor(cross_train[, 2:53])
highlyCorrelated <- findCorrelation(correlationMatrix, cutoff=0.5)
cross_train_reduced <- cross_train[-c(1 +highlyCorrelated)]
print(colnames(cross_train[-c(1 +highlyCorrelated)]))
```
 
- Eliminating redundant (correlation > 0.75) features with {caret}'s *findCorrelation*: 34 predictors with 14414 samples; accuracy: 0.987. 
```{r redundant variables 75, echo=TRUE, message=FALSE, warning=FALSE}
corr75 <- findCorrelation(correlationMatrix, cutoff = 0.75)
colnames(cross_train)[1 + corr75]
```